{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bitsandbytes>=0.43.0 transformers>=4.38.0 peft>=0.7.0 accelerate>=0.25.0 trl>=0.7.4 datasets>=2.14.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:58:14.987188Z","iopub.execute_input":"2025-03-04T17:58:14.987458Z","iopub.status.idle":"2025-03-04T17:58:22.173333Z","shell.execute_reply.started":"2025-03-04T17:58:14.987434Z","shell.execute_reply":"2025-03-04T17:58:22.172240Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import sys\nimport os\nimport logging\nimport torch\nimport torch.distributed as dist\nimport importlib.util\nimport subprocess\nfrom accelerate import notebook_launcher\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    CPUOffload,\n    BackwardPrefetch,\n    ShardingStrategy,\n    StateDictType,\n    FullStateDictConfig,\n    OptimStateDictConfig,\n    FullOptimStateDictConfig,\n)\nimport transformers\nfrom transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm, LlamaRotaryEmbedding\nimport math\nimport torch.nn.init as init\nimport torch.nn as nn\nfrom functools import partial\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\nimport enum\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\n# Debug print helper function - define it early so it's available throughout\ndef debug_print(message):\n    print(f\"DEBUG: {message}\")\n    with open(\"fsdp_debug.log\", \"a\") as f:\n        f.write(f\"{message}\\n\")\n\n# Check if a package is installed\ndef is_package_installed(package_name):\n    \"\"\"Check if a package is installed\"\"\"\n    return importlib.util.find_spec(package_name) is not None\n\n# Install missing packages\ndef install_missing_packages():\n    \"\"\"Install required packages if missing\"\"\"\n    required_packages = [\n        \"bitsandbytes>=0.43.0\",\n        \"transformers>=4.38.0\",\n        \"peft>=0.7.0\",\n        \"accelerate>=0.25.0\",\n        \"trl>=0.7.4\",\n        \"datasets>=2.14.0\",\n    ]\n    \n    for package in required_packages:\n        package_name = package.split(\">=\")[0]\n        if not is_package_installed(package_name):\n            logger.info(f\"Installing {package}...\")\n            try:\n                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n                logger.info(f\"Successfully installed {package}\")\n            except subprocess.CalledProcessError as e:\n                logger.error(f\"Failed to install {package}: {e}\")\n                if package_name == \"bitsandbytes\":\n                    logger.info(\"Attempting to install bitsandbytes with specific CUDA version...\")\n                    try:\n                        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"bitsandbytes-cuda118\"])\n                    except:\n                        logger.error(\"Failed to install bitsandbytes-cuda118. Please install manually.\")\n\n# Set up environment variables\ndef setup_environment():\n    \"\"\"Set up environment variables for training\"\"\"\n    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n    os.environ[\"ACCELERATE_USE_FSDP\"] = \"1\"\n    os.environ[\"FSDP_CPU_RAM_EFFICIENT_LOADING\"] = \"1\"\n    os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"\n    logger.info(\"Environment variables set\")\n\n# Move the wrap policy function to module level for picklability\n# Define as a separate top-level function rather than a nested function\ndef peft_lora_wrap_policy(module, recurse, unwrapped_params, **kwargs):\n    \"\"\"Custom wrap policy that avoids wrapping LoRA modules in FSDP\"\"\"\n    # Explicitly avoid wrapping LoRA modules\n    if isinstance(module, (nn.Linear, nn.Embedding)) and \\\n       any(\"lora\" in name for name, _ in module.named_parameters(recurse=False)):\n        return False\n\n    # Force-wrap transformer layers\n    if isinstance(module, LlamaDecoderLayer):\n        return True\n\n    # Default to size-based policy for other modules\n    return size_based_auto_wrap_policy(\n        module, recurse, unwrapped_params,\n        min_num_params=100_000_000  # Only wrap large modules\n    )\n\n# Global formatting function needed by the trainer\ndef formatting_func(example):\n    return [example[\"text\"]]\n\n# Main training function for distributed execution\ndef training_function():\n    \"\"\"Main training function to be executed by notebook_launcher\"\"\"\n    # Get local rank for distributed training\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    world_size = int(os.environ.get(\"WORLD_SIZE\", \"1\"))\n    \n    logger.info(f\"Starting process with local_rank: {local_rank}, world_size: {world_size}\")\n    \n    # Set device and initialize process group\n    torch.cuda.set_device(local_rank)\n    if not dist.is_initialized():\n        logger.info(\"Initializing process group for distributed training...\")\n        dist.init_process_group(backend=\"nccl\")\n        logger.info(f\"Process group initialized: rank={dist.get_rank()}, world_size={dist.get_world_size()}\")\n\n    # Log system information\n    logger.info(f\"PyTorch version: {torch.__version__}\")\n    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        logger.info(f\"CUDA device count: {torch.cuda.device_count()}\")\n        logger.info(f\"Current CUDA device: {torch.cuda.current_device()}\")\n        logger.info(f\"Current CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n    \n    # Verify bitsandbytes installation\n    try:\n        import bitsandbytes as bnb\n        logger.info(f\"Successfully imported bitsandbytes version: {bnb.__version__}\")\n    except ImportError:\n        logger.error(\"Failed to import bitsandbytes. Attempting reinstallation...\")\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"bitsandbytes>=0.43.0\"])\n            import bitsandbytes as bnb\n            logger.info(f\"Reinstalled and imported bitsandbytes version: {bnb.__version__}\")\n        except:\n            logger.error(\"Failed to reinstall bitsandbytes. Training may fail.\")\n    \n    # Setup model and tokenizer\n    logger.info(\"Setting up model and tokenizer...\")\n    dtype = torch.float16\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=dtype,\n        bnb_4bit_quant_storage=dtype,  # For FSDP compatibility\n    )\n    \n    logger.info(\"Loading model...\")\n    model_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            torch_dtype=dtype,\n            device_map=None,  # Important for FSDP\n            attn_implementation=\"sdpa\",\n            low_cpu_mem_usage=True,\n        )\n    except Exception as e:\n        logger.error(f\"Error loading model: {e}\")\n        try:\n            logger.info(\"Attempting to load with alternate configuration...\")\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=dtype,\n                device_map=None,\n                attn_implementation=\"sdpa\",\n                low_cpu_mem_usage=True,\n            )\n        except Exception as e2:\n            logger.error(f\"Failed with alternate config too: {e2}\")\n            return\n    \n    logger.info(\"Loading tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.padding_side = \"right\"\n    \n    # Define mixed precision and CPU offloading policies\n    mixed_precision_policy = MixedPrecision(\n        param_dtype=dtype,\n        reduce_dtype=dtype,\n        buffer_dtype=dtype,\n    )\n    cpu_offload = CPUOffload(offload_params=True)\n    \n    # Set up LoRA\n    logger.info(\"Setting up LoRA configuration...\")\n    lora_config = LoraConfig(\n        r=32,              # Reduced for T4 GPUs\n        lora_alpha=64,     # Reduced for T4 compatibility\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_dropout=0,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM,\n    )\n    \n    logger.info(\"Applying LoRA to model...\")\n    model = get_peft_model(model, lora_config)\n    \n    # Set only LoRA parameters to trainable\n    logger.info(\"Setting trainable parameters...\")\n    trainable_param_count, total_param_count = 0, 0\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            total_param_count += param.numel()\n            if \".lora_A.\" in name or \".lora_B.\" in name:\n                param.requires_grad_(True)\n                trainable_param_count += param.numel()\n            else:\n                param.requires_grad_(False)\n    logger.info(f\"Trainable parameters: {trainable_param_count:,} ({trainable_param_count/total_param_count:.2%} of total {total_param_count:,})\")\n    \n    # Enable gradient checkpointing\n    logger.info(\"Enabling gradient checkpointing...\")\n    model.gradient_checkpointing_enable()\n    model.enable_input_require_grads()\n    \n    # Patch LlamaRMSNorm and LlamaRotaryEmbedding if needed\n    if not hasattr(LlamaRMSNorm, 'reset_parameters'):\n        def reset_parameters(self):\n            pass\n        LlamaRMSNorm.reset_parameters = reset_parameters\n        logger.info(\"Successfully added reset_parameters method to LlamaRMSNorm\")\n    if not hasattr(LlamaRotaryEmbedding, 'reset_parameters'):\n        def reset_parameters(self):\n            pass\n        LlamaRotaryEmbedding.reset_parameters = reset_parameters\n        logger.info(\"Successfully added reset_parameters method to LlamaRotaryEmbedding\")\n    \n    # Patch Linear layers to safely handle quantized weights\n    def patch_module_with_quantized_safety(module_class):\n        if hasattr(module_class, 'reset_parameters'):\n            original_reset_parameters = module_class.reset_parameters\n            def safe_reset_parameters(self):\n                skip_init = False\n                if hasattr(self, 'weight') and self.weight is not None:\n                    if self.weight.dtype in [torch.uint8, torch.int8, torch.quint8, torch.qint8]:\n                        logger.info(f\"Skipping initialization for quantized weights in {self}\")\n                        skip_init = True\n                if not skip_init:\n                    original_reset_parameters(self)\n            module_class.reset_parameters = safe_reset_parameters\n            return True\n        else:\n            def empty_reset_parameters(self):\n                pass\n            module_class.reset_parameters = empty_reset_parameters\n            return False\n\n    for module_class in [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Embedding]:\n        had_reset = patch_module_with_quantized_safety(module_class)\n        if had_reset:\n            logger.info(f\"Patched {module_class.__name__}.reset_parameters to handle quantized weights\")\n        else:\n            logger.info(f\"Added empty reset_parameters to {module_class.__name__}\")\n    \n    # Convert frozen quantized parameters to buffers\n    logger.info(\"Converting frozen quantized parameters to buffers...\")\n    for module in model.modules():\n        for name, param in list(module.named_parameters(recurse=False)):\n            if not param.requires_grad and param.dtype in [torch.uint8, torch.int8, torch.quint8, torch.qint8]:\n                logger.info(f\"Converting {name} in {module} from parameter to buffer\")\n                module._parameters.pop(name)\n                module.register_buffer(name, param)\n    \n    # Mark quantized parameters to be ignored by FSDP\n    for name, param in model.named_parameters():\n        if param.dtype in [torch.uint8, torch.int8, torch.quint8, torch.qint8]:\n            setattr(param, '_fsdp_ignore', True)\n    \n    logger.info(\"Casting model to torch.float16 for uniform dtypes...\")\n    model = model.half()\n    \n    max_seq_length = 512  # Reduced for T4 GPUs\n    logger.info(f\"Using max sequence length: {max_seq_length}\")\n    \n    # Load and prepare dataset\n    logger.info(\"Loading dataset...\")\n    try:\n        url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n        dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:2%]\")\n        def format_text(example):\n            text = example[\"text\"]\n            if isinstance(text, list):\n                return {\"text\": \" \".join([str(t) for t in text])}\n            return {\"text\": str(text)}\n        dataset = dataset.map(format_text, remove_columns=[col for col in dataset.column_names if col != \"text\"])\n        def tokenize_function(examples):\n            return tokenizer(\n                examples[\"text\"],\n                padding=\"max_length\",\n                truncation=True,\n                max_length=max_seq_length,\n                return_tensors=\"pt\",\n            )\n        dataset = dataset.map(\n            tokenize_function,\n            batched=True,\n            remove_columns=[\"text\"],\n            desc=\"Tokenizing dataset\",\n        )\n    except Exception as e:\n        logger.error(f\"Error loading dataset: {e}\")\n        import traceback\n        logger.error(traceback.format_exc())\n        return\n\n    # Configure training with FSDP2 settings using the global wrap policy\n    debug_print(\"Setting up FSDP2 configuration...\")\n    \n    # Using our top-level function for picklability\n    debug_print(\"Using top-level peft_lora_wrap_policy function for picklability\")\n    \n    fsdp_config = {\n        \"fsdp_transformer_layer_cls_to_wrap\": [\"LlamaDecoderLayer\"],\n        \"fsdp_sharding_strategy\": ShardingStrategy.HYBRID_SHARD,\n        \"fsdp_offload_params\": False,\n        \"fsdp_activation_checkpointing\": True,\n        \"fsdp_use_orig_params\": True,\n        \"fsdp_auto_wrap_policy\": peft_lora_wrap_policy,  # Use the module-level function\n        \"fsdp_sync_module_states\": True,\n        \"fsdp_cpu_ram_efficient_loading\": True,\n        \"fsdp_state_dict_type\": StateDictType.SHARDED_STATE_DICT,\n        \"state_dict_config\": FullStateDictConfig(offload_to_cpu=True, rank0_only=True),\n        \"optim_state_dict_config\": FullOptimStateDictConfig(offload_to_cpu=True),\n    }\n    \n    debug_print(\"FSDP config created, checking if wrap policy is picklable...\")\n    import pickle\n    try:\n        pickle.dumps(fsdp_config)\n        debug_print(\"FSDP config is picklable\")\n    except Exception as e:\n        debug_print(f\"FSDP config is not picklable: {e}\")\n        debug_print(\"Trying to identify unpicklable components...\")\n        for key, value in fsdp_config.items():\n            try:\n                pickle.dumps(value)\n                #debug_print(f\"  - {key} is picklable\")\n            except Exception as e:\n                logger.info(f\"Error: {e}\")\n                #debug_print(f\"  - {key} is not picklable: {e}\")\n    \n    training_args = SFTConfig(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=1,\n        max_steps=20,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        seed=3407,\n        max_seq_length=max_seq_length,\n        fp16=dtype == torch.float16,\n        bf16=dtype == torch.bfloat16,\n        report_to=\"none\",\n        save_steps=10,\n        save_total_limit=2,\n        save_strategy=\"steps\",\n        fsdp=\"full_shard auto_wrap offload\",\n        fsdp_config=fsdp_config,\n        remove_unused_columns=False,\n        dataloader_num_workers=0,\n        disable_tqdm=False,\n        logging_first_step=True,\n        dataset_num_proc=1,\n        dataloader_drop_last=False,\n        dataloader_pin_memory=False,\n        auto_find_batch_size=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n        optim=\"adamw_torch\",\n        fp16_full_eval=False,\n        half_precision_backend=\"cuda_amp\",\n    )\n\n    debug_print(\"Initializing trainer...\")\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        args=training_args,\n        tokenizer=tokenizer,\n        data_collator=transformers.DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False,\n            pad_to_multiple_of=8\n        ),\n        formatting_func=formatting_func,\n    )\n\n    # Debug: Inspect model parameters and FSDP structure\n    debug_print(\"===== CHECKING MODEL PARAMETERS BEFORE TRAINING =====\")\n    total_params = 0\n    trainable_params = 0\n    buffer_count = 0\n    quantized_params = 0\n    for name, param in model.named_parameters():\n        total_params += 1\n        if param.requires_grad:\n            trainable_params += 1\n        if hasattr(param, '_fsdp_ignore') and param._fsdp_ignore:\n            logger.info(f\"Parameter marked for FSDP ignore: {name}\")\n            #debug_print(f\"Parameter marked for FSDP ignore: {name}\")\n        if param.dtype in [torch.uint8, torch.int8, torch.quint8, torch.qint8]:\n            quantized_params += 1\n            #debug_print(f\"Quantized parameter found: {name} with dtype {param.dtype}\")\n    for name, buffer in model.named_buffers():\n        buffer_count += 1\n        if buffer.dtype in [torch.uint8, torch.int8, torch.quint8, torch.qint8]:\n            logger.info(f\"Quantized buffer found: {name} with dtype {buffer.dtype}\")\n            #debug_print(f\"Quantized buffer found: {name} with dtype {buffer.dtype}\")\n    debug_print(f\"Total parameters: {total_params}, Trainable: {trainable_params}, Buffers: {buffer_count}, Quantized params: {quantized_params}\")\n    \n    # Completely overhaul the checkpoint saving approach\n    original_save_checkpoint = trainer._save_checkpoint\n    \n    def custom_save_checkpoint(model, trial, metrics=None):\n        \"\"\"Custom checkpoint saving function that handles FSDP state dict issues\"\"\"\n        debug_print(\"===== CUSTOM CHECKPOINT SAVING =====\")\n        \n        # Only process on the main process\n        if dist.get_rank() != 0:\n            debug_print(\"Not rank 0, skipping checkpoint save\")\n            return\n            \n        debug_print(\"Saving checkpoint on rank 0\")\n        output_dir = os.path.join(trainer.args.output_dir, f\"checkpoint-{trainer.state.global_step}\")\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # 1. Save the training arguments WITHOUT the wrap policy\n        debug_print(\"Saving training arguments without wrap policy\")\n        args_dict = trainer.args.to_dict()\n        \n        # Convert non-serializable objects to strings\n        def make_json_serializable(obj):\n            if isinstance(obj, dict):\n                return {k: make_json_serializable(v) for k, v in obj.items()}\n            elif isinstance(obj, (list, tuple)):\n                return [make_json_serializable(item) for item in obj]\n            elif isinstance(obj, enum.Enum):\n                return str(obj.name)  # Convert enum to string\n            elif hasattr(obj, '__name__'):\n                return obj.__name__  # Function or class name\n            elif hasattr(obj, '__class__'):\n                return str(obj)  # General fallback for objects\n            else:\n                return obj\n        \n        # Process the args dict to make it serializable\n        processed_args = make_json_serializable(args_dict)\n        debug_print(\"Processed training args to make them JSON serializable\")\n        \n        import json\n        with open(os.path.join(output_dir, \"training_args.json\"), \"w\") as f:\n            json.dump(processed_args, f, indent=2)\n        debug_print(\"Saved training arguments to JSON\")\n            \n        # 2. Save the model state itself - directly use PEFT for simplicity\n        try:\n            debug_print(\"Saving model using PEFT's save_pretrained\")\n            model.save_pretrained(output_dir)\n            debug_print(\"Successfully saved model state using PEFT\")\n        except Exception as e:\n            debug_print(f\"Error saving model state: {e}\")\n            import traceback\n            debug_print(traceback.format_exc())\n            \n        # 3. Save tokenizer and other components\n        trainer.tokenizer.save_pretrained(output_dir)\n        debug_print(\"Saved tokenizer\")\n        \n        # 4. Save training state (simplified to avoid serialization issues)\n        if hasattr(trainer, \"state\"):\n            # Convert state to dict and make it serializable\n            state_dict = {k: v for k, v in trainer.state.__dict__.items()}\n            state_dict = make_json_serializable(state_dict)\n            with open(os.path.join(output_dir, \"trainer_state.json\"), \"w\") as f:\n                json.dump(state_dict, f, indent=2)\n            debug_print(\"Saved trainer state\")\n        \n        # 5. Save metrics if provided\n        if metrics:\n            metrics = make_json_serializable(metrics)\n            with open(os.path.join(output_dir, \"metrics.json\"), \"w\") as f:\n                json.dump(metrics, f, indent=2)\n            debug_print(\"Saved metrics\")\n        \n        debug_print(\"Checkpoint saving complete\")\n        return output_dir\n\n    # Replace the trainer's checkpoint save method with our custom one\n    debug_print(\"Registering custom checkpoint saving method\")\n    trainer._save_checkpoint = custom_save_checkpoint\n\n    debug_print(\"===== STARTING TRAINING =====\")\n    try:\n        trainer.train()\n        if dist.get_rank() == 0:\n            debug_print(\"Training complete! Saving final model...\")\n            # Use PEFT's save_pretrained which handles LoRA weights correctly\n            model.save_pretrained(os.path.join(trainer.args.output_dir, \"final_model\"))\n            tokenizer.save_pretrained(os.path.join(trainer.args.output_dir, \"final_model\"))\n            debug_print(\"Final model saved successfully.\")\n    except Exception as e:\n        debug_print(f\"Error during training: {e}\")\n        import traceback\n        debug_print(traceback.format_exc())\n        raise\n\n# Main execution script for Kaggle notebook\ndef main():\n    install_missing_packages()\n    setup_environment()\n    try:\n        from accelerate import notebook_launcher\n    except ImportError:\n        logger.error(\"accelerate library is missing. Installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"accelerate>=0.25.0\"])\n        from accelerate import notebook_launcher\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    logger.info(\"Launching training across 2 GPUs...\")\n    notebook_launcher(training_function, num_processes=2)\n    logger.info(\"Training complete!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T18:02:43.352882Z","iopub.execute_input":"2025-03-04T18:02:43.353176Z","iopub.status.idle":"2025-03-04T18:12:48.501493Z","shell.execute_reply.started":"2025-03-04T18:02:43.353153Z","shell.execute_reply":"2025-03-04T18:12:48.500230Z"}},"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"041fe1b8fc2a4a84a86fb212082ef2cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ca09e024854f1c91ce69014da8965a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7e8e4de8024e47b3c398818b3e71a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17bb2b306dd4a0daecd5e332bd1a4c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3b97a9c78842688bbf3362160479c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unified_chip2.jsonl:   0%|          | 0.00/95.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86cbc933fd3843fbadcdbd8ec355354e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d6c65007e0424c9dbdc1b949898a74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67a4c65b9c16489babc68e4c895c5996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/4206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0cab28438d54479ac3cd39bdfc4d539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/4206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09017d62dc1e43afb4bda115a0d720a7"}},"metadata":{}},{"name":"stdout","text":"DEBUG: Setting up FSDP2 configuration...\nDEBUG: Using top-level peft_lora_wrap_policy function for picklability\nDEBUG: FSDP config created, checking if wrap policy is picklable...\nDEBUG: FSDP config is picklable\nDEBUG: Initializing trainer...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-0f949d514a91>:391: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n","output_type":"stream"},{"name":"stdout","text":"DEBUG: Setting up FSDP2 configuration...\nDEBUG: Using top-level peft_lora_wrap_policy function for picklability\nDEBUG: FSDP config created, checking if wrap policy is picklable...\nDEBUG: FSDP config is picklable\nDEBUG: Initializing trainer...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-0f949d514a91>:391: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:381: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/4206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d1daac69814770aacbd4d6d9090dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/4206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e65110f22ac478bbbb0819672c3c183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/4206 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f73a5c5475547a0b4dc2eb89558453f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:381: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"DEBUG: ===== CHECKING MODEL PARAMETERS BEFORE TRAINING =====\nDEBUG: Total parameters: 515, Trainable: 448, Buffers: 257, Quantized params: 0\nDEBUG: Registering custom checkpoint saving method\nDEBUG: ===== STARTING TRAINING =====\nDEBUG: ===== CHECKING MODEL PARAMETERS BEFORE TRAINING =====\nDEBUG: Total parameters: 515, Trainable: 448, Buffers: 257, Quantized params: 0\nDEBUG: Registering custom checkpoint saving method\nDEBUG: ===== STARTING TRAINING =====\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:1585: UserWarning: Upcasted low precision parameters in Linear because mixed precision turned on in FSDP. Affects: weight.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:1591: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11/20 06:36 < 06:36, 0.02 it/s, Epoch 0.04/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>8.506900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.983400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>8.814600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7.820100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>10.542300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>8.185400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>7.734000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>9.109900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>8.168100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11/20 06:36 < 06:36, 0.02 it/s, Epoch 0.04/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>8.506900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.983400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>8.814600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7.820100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>10.542300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>8.185400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>7.734000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>9.109900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>8.168100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"DEBUG: ===== CUSTOM CHECKPOINT SAVING =====DEBUG: ===== CUSTOM CHECKPOINT SAVING =====\n\nDEBUG: Saving checkpoint on rank 0DEBUG: Not rank 0, skipping checkpoint save\n\nDEBUG: Saving training arguments without wrap policy\nDEBUG: Processed training args to make them JSON serializable\nDEBUG: Saved training arguments to JSON\nDEBUG: Saving model using PEFT's save_pretrained\n","output_type":"stream"},{"name":"stderr","text":"W0304 18:12:16.128000 31 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers\nW0304 18:12:16.132000 31 torch/distributed/elastic/multiprocessing/api.py:766] Closing process 89 via signal SIGINT\nW0304 18:12:16.133000 31 torch/distributed/elastic/multiprocessing/api.py:766] Closing process 90 via signal SIGINT\nW0304 18:12:46.164000 31 torch/distributed/elastic/multiprocessing/api.py:783] Unable to shutdown process 89 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\nW0304 18:12:47.262000 31 torch/distributed/elastic/multiprocessing/api.py:783] Unable to shutdown process 90 via Signals.SIGINT, forcefully exiting via Signals.SIGKILL\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-0f949d514a91>\u001b[0m in \u001b[0;36m<cell line: 539>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-0f949d514a91>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MASTER_PORT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"29500\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Launching training across 2 GPUs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m     \u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py\u001b[0m in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_torch_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                         \u001b[0mlaunch_config_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_line_prefix_template\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_line_prefix_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     \u001b[0melastic_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLaunchConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlaunch_config_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrypoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProcessRaisedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Cannot re-initialize CUDA in forked subprocess\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlaunch_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entrypoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\u001b[0m in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetricsConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;31m# records that agent.run() has succeeded NOT that workers have succeeded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_succeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mput_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key}.success\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mshutdown_called\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_execution_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\u001b[0m in \u001b[0;36m_invoke_run\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mWorkerState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             \u001b[0mrun_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitor_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\u001b[0m in \u001b[0;36m_terminate_process_handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m     83\u001b[0m     \u001b[0msigval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSignals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSignalException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Process {os.getpid()} got signal: {sigval}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSignalException\u001b[0m: Process 31 got signal: 2"],"ename":"SignalException","evalue":"Process 31 got signal: 2","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}